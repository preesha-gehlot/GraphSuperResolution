{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "from preprocessing import *\n",
    "from model import *\n",
    "from model_deep import PerfectFeatureModel\n",
    "from types import SimpleNamespace\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA RTX A6000\n",
      "Total Memory: 44.6GB\n",
      "Allocated Memory: 0.0GB\n",
      "Free Memory: 44.6GB\n",
      "Using GPU 0 with 44.6GB free VRAM\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    for i in range(num_devices):\n",
    "        device = torch.cuda.device(i)\n",
    "        total_mem = torch.cuda.get_device_properties(i).total_memory / 1024**3  # Convert to GB\n",
    "        allocated_mem = torch.cuda.memory_allocated(i) / 1024**3  # Convert to GB\n",
    "        free_mem = total_mem - allocated_mem\n",
    "        \n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"Total Memory: {total_mem:.1f}GB\")\n",
    "        print(f\"Allocated Memory: {allocated_mem:.1f}GB\")\n",
    "        print(f\"Free Memory: {free_mem:.1f}GB\")\n",
    "        \n",
    "        if free_mem < 8:\n",
    "            print(f\"Warning: GPU {i} has less than 8GB of free VRAM!\")\n",
    "        else:\n",
    "            print(f\"Using GPU {i} with {free_mem:.1f}GB free VRAM\")\n",
    "            break \n",
    "    device = torch.device(f\"cuda:{0}\")\n",
    "else:\n",
    "    print(\"Warning: No CUDA devices available - running on CPU only\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anti_vec(vec):\n",
    "    N = math.ceil(math.sqrt(vec.shape[-1] * 2))\n",
    "    adj = torch.zeros((vec.shape[0], N, N), dtype=vec.dtype)\n",
    "    row_idx, col_idx = torch.triu_indices(N, N, offset=1)\n",
    "    row_idx = (N - 1 - row_idx).flip(dims=[0])\n",
    "    col_idx = (N - 1 - col_idx).flip(dims=[0])\n",
    "    adj[:, row_idx, col_idx] = vec\n",
    "    adj[:, col_idx, row_idx] = vec\n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.loadtxt('../lr_train.csv', delimiter=',', skiprows=1)\n",
    "training_data_adj = anti_vec(torch.from_numpy(training_data))\n",
    "\n",
    "training_label = np.loadtxt('../hr_train.csv', delimiter=',', skiprows=1)\n",
    "training_label_adj = anti_vec(torch.from_numpy(training_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = math.ceil(math.sqrt(training_data.shape[-1] * 2))\n",
    "n_prime =  math.ceil(math.sqrt(training_label.shape[-1] * 2))\n",
    "\n",
    "args = {\n",
    "    \"lr_dim\": 160,\n",
    "    \"hr_dim\": 268,\n",
    "    \"hidden_dim\": 268,\n",
    "    \"lr\": 0.005,\n",
    "    \"epochs\": 100,\n",
    "    \"padding\": 26,\n",
    "    \"device\": device,\n",
    "    \"batch_size\": len(training_data) // 10\n",
    "}\n",
    "args = SimpleNamespace(**args)\n",
    "model = PerfectFeatureModel(160, 268).to(device)\n",
    "state_dict = torch.load(f\"./model_{0}/model.pth\")\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2780730/1571331060.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Memory Allocated: 2116.49 MB\n",
      "Average Time per Sample: 0.004427226003772484 s\n"
     ]
    }
   ],
   "source": [
    "total_time = 0.0\n",
    "max_memory_use = 0.0\n",
    "for x, y in zip(training_data_adj, training_label_adj):\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    initial_memory = torch.cuda.memory_allocated()\n",
    "    x = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "    start_time = time.time()\n",
    "    pred, _ = model(x)\n",
    "    total_time += time.time() - start_time\n",
    "    max_memory_use = max(max_memory_use, torch.cuda.max_memory_allocated() - initial_memory)\n",
    "total_time /= len(training_data_adj)\n",
    "print(f\"Max Memory Allocated: {max_memory_use / 1024**2:.2f} MB\")\n",
    "print(f\"Average Time per Sample: {total_time} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2780730/1961291804.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Time per Sample: 0.6095667627757181 s\n"
     ]
    }
   ],
   "source": [
    "model = model.to('cpu')\n",
    "total_time = 0.0\n",
    "for x, y in zip(training_data_adj, training_label_adj):\n",
    "    x = torch.tensor(x, dtype=torch.float32)\n",
    "    start_time = time.time()\n",
    "    pred, _ = model(x)\n",
    "    total_time += time.time() - start_time\n",
    "total_time /= len(training_data_adj)\n",
    "print(f\"Average Time per Sample: {total_time} s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
